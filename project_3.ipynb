{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Recurrent Neural Networks\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    Welcome to Project 3: Recurrent Neural Networks!<br>\n",
    "    <br>\n",
    "    In this project you will work with a basic RNN architecture for text generation, namely Sequence to Seqeuence Models. The name points out that these models take a sequence as input and return another sequence as output (see Fig. 1).<br>\n",
    "    <br>\n",
    "    Sequence to Vector Models are similar, but return only a single output in the final time step (see Fig. 2). The latter architecture is, for example, suitable for sequence classification.\n",
    "</div>\n",
    "<br>\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <th><img src=\"rnn_seq_2_seq.png?666\" alt=\"\" style=\"width: 475px;\"></th>\n",
    "        <th><img src=\"rnn_seq_2_vec.png?666\" alt=\"\" style=\"width: 475px;\"></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Fig. 1: Sequence to Sequence RNN with one hidden layer.</th>\n",
    "        <th>Fig. 2: Sequence to Vector RNN with one hidden layer.</th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Read the text files <font face='courier'>hamlet_1.txt</font>, <font face='courier'>hamlet_2.txt</font> and <font face='courier'>hamlet_3.txt</font>.<br>\n",
    "    <br>\n",
    "    Store their content in variables <font face='courier'>hamlet_1_text</font>, <font face='courier'>hamlet_2_text</font> and <font face='courier'>hamlet_3_text</font>, respectively.<br>\n",
    "    <br>\n",
    "    Be aware that all files are UTF-8 encoded. Maybe you find <a href='https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files'>this link</a> helpful.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "def read_txt(path):\n",
    "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "hamlet_1_text = read_txt('hamlet_1.txt')\n",
    "hamlet_2_text = read_txt('hamlet_2.txt')\n",
    "hamlet_3_text = read_txt('hamlet_3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Print the first 325 characters of <font face='courier'>hamlet_1_text</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_text[:255]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should see the following output:\n",
    "\n",
    "```\n",
    "The Tragedie of Hamlet\n",
    "\n",
    "Actus Primus. Scoena Prima.\n",
    "\n",
    "Enter Barnardo and Francisco two Centinels.\n",
    "\n",
    " Barnardo. Who's there?\n",
    "Fran. Nay answer me: Stand & vnfold\n",
    "your selfe\n",
    "\n",
    " Bar. Long liue the King\n",
    "\n",
    " Fran. Barnardo?\n",
    "Bar. He\n",
    "\n",
    " Fran. You come most carefully vpon your houre\n",
    "\n",
    " Bar. 'Tis now strook twelue, get thee to bed Francisco\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Instantiate an object <font face='courier'>tokenizer</font> of type <font face='courier'>tf.keras.preprocessing.text.Tokenzier</font>.<br>\n",
    "    <br>\n",
    "    Submit the argument <font face='courier'>char_level=True</font> to the constructor.<br>\n",
    "    <br>\n",
    "    Helpful information can be found <a href='https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer'>here</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The object <font face='courier'>tokenizer</font> will ultimately be used to encode the strings <font face='courier'>hamlet_1_text</font>, <font face='courier'>hamlet_2_text</font> and <font face='courier'>hamlet_3_text</font> as integer sequences in which each single number represents a specific character.<br>\n",
    "    <br>\n",
    "    Call the method <font face='courier'>fit_on_texts</font> of <font face='courier'>tokenizer</font>.<br>\n",
    "    <br>\n",
    "    In doing so, pass a list that contains exactly the strings <font face='courier'>hamlet_1_text</font>, <font face='courier'>hamlet_2_text</font> and <font face='courier'>hamlet_3_text</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "tokenizer.fit_on_texts([hamlet_1_text, hamlet_2_text, hamlet_3_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The above call creates a vocabulary in <font face='courier'>tokenizer</font>. This vocabulary assigns a positive integer to each unique character in the texts that were passed to the <font face='courier'>fit_on_texts</font> method.<br>\n",
    "    <br>\n",
    "    Display the attribute <font face='courier'>word_index</font> of <font face='courier'>tokenizer</font> which contains the created vocabulary.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should have got the following output:\n",
    "\n",
    "```\n",
    "{' ': 1, 'e': 2, 't': 3, 'o': 4, 'a': 5, 'h': 6, 'i': 7, 'n': 8, 's': 9, 'r': 10, 'l': 11, '\\n': 12, 'u': 13, 'd': 14, 'm': 15, 'y': 16, ',': 17, 'w': 18, 'f': 19, 'c': 20, 'g': 21, '.': 22, 'p': 23, 'b': 24, 'k': 25, \"'\": 26, ':': 27, 'v': 28, '?': 29, ';': 30, 'q': 31, 'x': 32, '-': 33, 'z': 34, '(': 35, ')': 36, '&': 37, '!': 38, '[': 39, ']': 40, '1': 41, 'j': 42}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Compute the length of the vocabulary, store it in the variable <font face='courier'>max_id</font> and display this variable.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "max_id = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Now, use the method <font face='courier'>texts_to_sequences</font> of <font face='courier'>tokenzier</font> to encode <font face='courier'>hamlet_1_text</font>, <font face='courier'>hamlet_2_text</font> and <font face='courier'>hamlet_3_text</font>.<br>\n",
    "    <br>\n",
    "    Store the respective coded strings in <font face='courier'>hamlet_1_encoded</font>, <font face='courier'>hamlet_2_encoded</font> and <font face='courier'>hamlet_3_encoded</font>.<br>\n",
    "    <br>\n",
    "    Convert all three lists to the format <font face='courier'>numpy.array</font> and subtract <font face='courier'>1</font> from all entries so that all values range between <font face='courier'>0</font> and <font face='courier'>max_id - 1</font> afterwards.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_encoded = tokenizer.texts_to_sequences(hamlet_1_text)\n",
    "hamlet_2_encoded = tokenizer.texts_to_sequences(hamlet_2_text)\n",
    "hamlet_3_encoded = tokenizer.texts_to_sequences(hamlet_3_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Display the first <font face='courier'>325</font> entries of <font face='courier'>hamlet_1_encoded</font>.\n",
    "    <a href=''></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_encoded[:325]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Your output should be as follows:\n",
    "\n",
    "```\n",
    "[ 2  5  1  0  2  9  4 20  1 13  6  1  0  3 18  0  5  4 14 10  1  2 11 11\n",
    "  4 19  2 12  8  0 22  9  6 14 12  8 21  0  8 19  3  1  7  4  0 22  9  6\n",
    " 14  4 21 11 11  1  7  2  1  9  0 23  4  9  7  4  9 13  3  0  4  7 13  0\n",
    " 18  9  4  7 19  6  8 19  3  0  2 17  3  0 19  1  7  2  6  7  1 10  8 21\n",
    " 11 11  0 23  4  9  7  4  9 13  3 21  0 17  5  3 25  8  0  2  5  1  9  1\n",
    " 28 11 18  9  4  7 21  0  7  4 15  0  4  7  8 17  1  9  0 14  1 26  0  8\n",
    "  2  4  7 13  0 36  0 27  7 18  3 10 13 11 15  3 12  9  0  8  1 10 18  1\n",
    " 11 11  0 23  4  9 21  0 10  3  7 20  0 10  6 12  1  0  2  5  1  0 24  6\n",
    "  7 20 11 11  0 18  9  4  7 21  0 23  4  9  7  4  9 13  3 28 11 23  4  9\n",
    " 21  0  5  1 11 11  0 18  9  4  7 21  0 15  3 12  0 19  3 14  1  0 14  3\n",
    "  8  2  0 19  4  9  1 18 12 10 10 15  0 27 22  3  7  0 15  3 12  9  0  5\n",
    "  3 12  9  1 11 11  0 23  4  9 21  0 25  2  6  8  0  7  3 17  0  8  2  9\n",
    "  3  3 24  0  2 17  1 10 12  1 16  0 20  1  2  0  2  5  1  1  0  2  3  0\n",
    " 23  1 13  0 18  9  4  7 19  6  8 19  3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The original texts can be recovered via the method <font face='courier'>sequences_to_texts</font> of <font face='courier'>tokenizer</font>.<br>\n",
    "    <br>\n",
    "    Texts recovered in this way are all lower case and each original character is followed by a blank space.<br>\n",
    "    <br>\n",
    "    Apply <font face='courier'>sequences_to_texts</font> to <font face='courier'>hamlet_1_encoded + 1</font> and store the result in <font face='courier'>hamlet_1_decoded</font>.<br>\n",
    "    <br>\n",
    "    Afterfwards, display the first <font face='courier'>649</font> characters of <font face='courier'>hamlet_1_decoded</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_decoded = tokenizer.sequences_to_texts(hamlet_1_encoded±1)\n",
    "print(hamlet_1_decoded[:649])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Your output should look like this:\n",
    "\n",
    "```\n",
    "t h e   t r a g e d i e   o f   h a m l e t \n",
    " \n",
    " a c t u s   p r i m u s .   s c o e n a   p r i m a . \n",
    " \n",
    " e n t e r   b a r n a r d o   a n d   f r a n c i s c o   t w o   c e n t i n e l s . \n",
    " \n",
    "   b a r n a r d o .   w h o ' s   t h e r e ? \n",
    " f r a n .   n a y   a n s w e r   m e :   s t a n d   &   v n f o l d \n",
    " y o u r   s e l f e \n",
    " \n",
    "   b a r .   l o n g   l i u e   t h e   k i n g \n",
    " \n",
    "   f r a n .   b a r n a r d o ? \n",
    " b a r .   h e \n",
    " \n",
    "   f r a n .   y o u   c o m e   m o s t   c a r e f u l l y   v p o n   y o u r   h o u r e \n",
    " \n",
    "   b a r .   ' t i s   n o w   s t r o o k   t w e l u e ,   g e t   t h e e   t o   b e d   f r a n c i s c o\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Create three objects <font face='courier'>hamlet_1_dataset</font>, <font face='courier'>hamlet_2_dataset</font> and <font face='courier'>hamlet_3_dataset</font> of type <font face='courier'>tf.data.dataset</font> by applying the method<br>\n",
    "    <br>\n",
    "    <font face='courier'>tf.data.Dataset.from_tensor_slices</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices'>this link</a>) to <font face='courier'>hamlet_1_encoded</font>, <font face='courier'>hamlet_2_encoded</font> and <font face='courier'>hamlet_3_encoded</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_dataset = tf.data.Dataset.from_tensor_slices(hamlet_1_encoded)\n",
    "hamlet_2_dataset = tf.data.Dataset.from_tensor_slices(hamlet_2_encoded)\n",
    "hamlet_3_dataset = tf.data.Dataset.from_tensor_slices(hamlet_3_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Display the first ten elements of <font face='courier'>hamlet_1_dataset</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_dataset.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should have produced the following output:\n",
    "\n",
    "```\n",
    "tf.Tensor(2, shape=(), dtype=int32)\n",
    "tf.Tensor(5, shape=(), dtype=int32)\n",
    "tf.Tensor(1, shape=(), dtype=int32)\n",
    "tf.Tensor(0, shape=(), dtype=int32)\n",
    "tf.Tensor(2, shape=(), dtype=int32)\n",
    "tf.Tensor(9, shape=(), dtype=int32)\n",
    "tf.Tensor(4, shape=(), dtype=int32)\n",
    "tf.Tensor(20, shape=(), dtype=int32)\n",
    "tf.Tensor(1, shape=(), dtype=int32)\n",
    "tf.Tensor(13, shape=(), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    As you can see, each item of <font face='courier'>hamlet_1_dataset</font> is an integer tensor including one single value.<br>\n",
    "    <br>\n",
    "    Accordingly, <font face='courier'>hamlet_1_dataset</font> has <font face='courier'>len(hamlet_1_encoded)</font> elements in total (the same applies in case of the other two datasets).<br>\n",
    "    <br>\n",
    "    In the following, you will train a recurrent neural network which gets a coded string of length <font face='courier'>T = 100</font> and predicts subsequent characters in all time steps.<br>\n",
    "    <br>\n",
    "    Accordingly, we will first transform our three datasets such that their elements become one-dimensional tensors of length <font face='courier'>window_length = T + 1</font>.<br>\n",
    "    <br>\n",
    "    Initialize <font face='courier'>T</font> and <font face='courier'>window_length</font> as described.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Use the method <font face='courier'>tf.data.Dataset.window</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window'>this link</a>) to get items that feature the desired length.<br>\n",
    "    <br>\n",
    "    Call the method using the arguments <font face='courier'>shift = 1</font> and <font face='courier'>drop_remainder = True</font>.<br>\n",
    "    <br>\n",
    "    The first-mentioned argument <font face='courier'>shift = 1</font> ensures that the first item of the transformed dataset contains the elements <font face='courier'>0,...,window_length - 1</font> of the original dataset, the next item <font face='courier'>1,...,window_length</font>, and so on.<br>\n",
    "    <br>\n",
    "    In other words: A window of length <font face='courier'>window_length</font> slides with feed <font face='courier'>shift</font> over the original dataset and extracts sequence by sequence until the end of the dataset is reached.<br>\n",
    "    <br>\n",
    "    The latter argument <font face='courier'>drop_remainder = True</font> ensures that no shorter sequences are extracted towards the end of the dataset, where the window could potentially slide beyond the end of the dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Execute the following code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for window in hamlet_1_dataset.take(1):\n",
    "    print(window)\n",
    "    for item in window.take(10):\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should have obtained an output like this:\n",
    "\n",
    "```\n",
    "<_VariantDataset shapes: (), types: tf.int32>\n",
    "tf.Tensor(2, shape=(), dtype=int32)\n",
    "tf.Tensor(5, shape=(), dtype=int32)\n",
    "tf.Tensor(1, shape=(), dtype=int32)\n",
    "tf.Tensor(0, shape=(), dtype=int32)\n",
    "tf.Tensor(2, shape=(), dtype=int32)\n",
    "tf.Tensor(9, shape=(), dtype=int32)\n",
    "tf.Tensor(4, shape=(), dtype=int32)\n",
    "tf.Tensor(20, shape=(), dtype=int32)\n",
    "tf.Tensor(1, shape=(), dtype=int32)\n",
    "tf.Tensor(13, shape=(), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    As you can see, the transformed datasets <font face='courier'>hamlet_1_dataset</font>, <font face='courier'>hamlet_2_dataset</font> and <font face='courier'>hamlet_3_dataset</font> have now elements which are again objects of type <font face='courier'>tf.data.Dataset</font> (or of a derived class).<br>\n",
    "    <br>\n",
    "    Each of these sub-datasets <font face='courier'>window</font> contains a number of <font face='courier'>window_size</font> single-valued tensors.<br>\n",
    "    <br>\n",
    "    Apply the method <font face='courier'>tf.data.Dataset.flat_map</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/data/Dataset#flat_map'>this link</a>) to all three datasets to transform the sub-datasets <font face='courier'>window</font> into one-dimensional tensors of length <font face='courier'>window_length</font>.<br>\n",
    "    <br>\n",
    "    Pass a function which maps <font face='courier'>window</font> to <font face='courier'>window.batch(window_length)</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Display the first element of <font face='courier'>hamlet_1_dataset</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should get the following output:\n",
    "\n",
    "```\n",
    "tf.Tensor(\n",
    "[ 2  5  1  0  2  9  4 20  1 13  6  1  0  3 18  0  5  4 14 10  1  2 11 11\n",
    "  4 19  2 12  8  0 22  9  6 14 12  8 21  0  8 19  3  1  7  4  0 22  9  6\n",
    " 14  4 21 11 11  1  7  2  1  9  0 23  4  9  7  4  9 13  3  0  4  7 13  0\n",
    " 18  9  4  7 19  6  8 19  3  0  2 17  3  0 19  1  7  2  6  7  1 10  8 21\n",
    " 11 11  0 23  4], shape=(101,), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Apply the method <font face='courier'>tf.data.Dataset.concatenate</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/data/Dataset#concatenate'>this link</a>) to merge<br>\n",
    "    <br>\n",
    "    <font face='courier'>hamlet_1_dataset</font>, <font face='courier'>hamlet_2_dataset</font> and <font face='courier'>hamlet_3_dataset</font> to a single dataset <font face='courier'>hamlet_dataset</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Set <font face='courier'>batch_size = 32</font>.<br>\n",
    "    <br>\n",
    "    Apply <font face='courier'>tf.data.Dataset.repeat</font> (without argument),<br>\n",
    "    <br>\n",
    "    <font face='courier'>tf.data.Dataset.shuffle</font> (with <font face='courier'>buffer_size = 1000</font>)<br>\n",
    "    <br>\n",
    "    and finally <font face='courier'>tf.data.Dataset.batch</font> (with <font face='courier'>drop_remainder=True</font>).\n",
    "    <a href=''></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Our dataset contains now two-dimensional tensors <font face='courier'>window_batch</font> of size <font face='courier'>(32, 101)</font>.<br>\n",
    "    <br>\n",
    "    Each slice <font face='courier'>window_batch[i, :]</font> corresponds to a training example.<br>\n",
    "    <br>\n",
    "    Here, we still need to subdivide the training examples into inputs and outputs.<br>\n",
    "    <br>\n",
    "    Each single encoded character <font face='courier'>window_batch[i, j]</font> (for <font face='courier'>j=0,...,99</font>) is an  input $\\mathbf{x}^{<t>(i)}$ in a time step<br>\n",
    "    <br>\n",
    "    and the associated output is <font face='courier'>window_batch[i, j + 1]</font> which corresponds to $\\mathbf{y}^{<t>(i)}$.<br>\n",
    "    <br>\n",
    "    Apply the method <font face='courier'>tf.data.Dataset.map</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map'>this link</a>) to <font face='courier'>hamlet_dataset</font><br>.\n",
    "    <br>\n",
    "    Each batch <font face='courier'>window_batch</font> shall be mapped to a tuple of two tensors of size <font face='courier'>(32, 100)</font>.<br>\n",
    "    <br>\n",
    "    The <font face='courier'>[i, :]</font>-th slice of the first tensor shall contain the entries <font face='courier'>window_batch[i, 0:100]</font>.<br>\n",
    "    <br>\n",
    "    The corresponding slice of the second tensor shall contain <font face='courier'>window_batch[i, 1:101]</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Execute the following code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window_batch in hamlet_dataset.take(1):\n",
    "    [x] = tokenizer.sequences_to_texts([window_batch[0][0, :].numpy() + 1])\n",
    "    [y] = tokenizer.sequences_to_texts([window_batch[1][0, :].numpy() + 1])\n",
    "    print(x)\n",
    "    print()\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Your output should look as follows:\n",
    "\n",
    "```\n",
    "  o f   y o u n g   f o r t i n b r a s , \n",
    " w h o   i m p o t e n t   a n d   b e d r i d ,   s c a r s e l y   h e a r e s \n",
    " o f   t h i s   h i s   n e p h e w e s   p u r p o s e ,   t o   s u p p\n",
    "\n",
    "o f   y o u n g   f o r t i n b r a s , \n",
    " w h o   i m p o t e n t   a n d   b e d r i d ,   s c a r s e l y   h e a r e s \n",
    " o f   t h i s   h i s   n e p h e w e s   p u r p o s e ,   t o   s u p p r\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Apply <font face='courier'>tf.data.Dataset.map</font> again to <font face='courier'>hamlet_dataset</font> to map each element <font face='courier'>(X_batch, Y_batch)</font> to a new tuple.<br>\n",
    "    <br>\n",
    "    In the new tuple, <font face='courier'>Y_batch</font> shall remain unchanged while <font face='courier'>X_batch</font> undergoes another encoding via <font face='courier'>tf.one_hot</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/one_hot'>this link</a>).<br>\n",
    "    <br>\n",
    "    Find out, which value <font face='courier'>depth</font> you need to pass <font face='courier'>tf.one_hot</font> in addition to <font face='courier'>X_batch</font>.<br>\n",
    "    <br>\n",
    "    Hint: You already computed the correct value above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Apply the method <font face='courier'>tf.data.Dataset.prefetch</font> (with <font face='courier'>buffer_size = 1</font>) to <font face='courier'>hamlet_dataset</font> to get your dataset ready for training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Deduce a formula for the number of elements in <font face='courier'>hamlet_dataset</font>.<br>\n",
    "    <br>\n",
    "    Display the result and store it in <font face='courier'>steps_per_epoch</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** The number of items in the dataset should be as follows:\n",
    "\n",
    "```\n",
    "5014\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Use <font face='courier'>keras.models.Sequential</font> to define a <font face='courier'>model</font> featuring<br>\n",
    "    <br>\n",
    "    two hidden GRU layers (see <a href='https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU'>this link</a>) with <font face='courier'>128</font> neurons each, as well as<br>\n",
    "    <br>\n",
    "    a fully connected output layer (see <a href='https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense'>this link</a>) with <font face='courier'>max_id</font> neurons and <font face='courier'>softmax</font> activation function.<br>\n",
    "    <br>\n",
    "    This model corresponds to the RNN displayed in Fig. 1 with an additional hidden GRU layer.<br>\n",
    "    <br>\n",
    "    To make the model generate outputs in each time step, the output layer must be enclosed by a <font face='courier'>keras.layers.TimeDistributed</font> wrapper (see <a href='https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed'>this link</a>).<br>\n",
    "    <br>\n",
    "    Without the layer, the model would generate only a single output in the very last time step (as the one displaye in Fig. 2).<br>\n",
    "    <br>\n",
    "    In case of the GRU layers, you should add <font face='courier'>return_sequences = True</font> for the same purpose.<br>\n",
    "    <br>\n",
    "    Also recall that the input layer needs an argument <font face='courier'>input_shape=[None, max_id]</font> (where <font face='courier'>None</font> represents the temporal dimension of the input sequence).<br>\n",
    "    <br>\n",
    "    During training, we could just as well replace <font face='courier'>None</font> with <font face='courier'>T</font> as all sequences in the training data have identical length.<br>\n",
    "    <br>\n",
    "    However, passing <font face='courier'>None</font> ensures that the model will accept arbitrarily long sequences later.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Compile <font face='courier'>model</font> using <font face='courier'>sparse_categorical_crossentropy</font> and <font face='courier'>adam</font>.\n",
    "    <a href=''></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Train <font face='courier'>model</font> for one epoch. Don't forget to pass the <font face='courier'>steps_per_epoch</font> argument.\n",
    "    <a href=''></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    You might have observed that even a single epoch takes quite some time.<br>\n",
    "    <br>\n",
    "    Create a file <font face='courier'>hamlet_rnn.py</font> and train the model for <font face='courier'>20</font> epochs on the GPU cluster.<br>\n",
    "    <br>\n",
    "    In doing so, use a callback<br>\n",
    "    <br>\n",
    "    <font face='courier'>keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)</font><br>\n",
    "    <br>\n",
    "    (see <a href='https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping'>this link</a>). Save your model as  <font face='courier'>hamlet_model.h5</font> and load it in the following step for further use in this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Write a function <font face='courier'>preprocess</font> that takes a argument a list <font face='courier'>texts</font> containing a single string.<br>\n",
    "    <br>\n",
    "    Inside this function, use <font face='courier'>tokenizer</font> again to encode the string in <font face='courier'>texts</font> as a NumPy-Array <font face='courier'>X</font>.<br>\n",
    "    <br>\n",
    "    The return value of <font face='courier'>preprocess</font> shall be the one-hot encoded version of <font face='courier'>X</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The function <font face='courier'>next_char</font> takes as argument a string <font face='courier'>text</font> and a positive number <font face='courier'>temperature</font><br>\n",
    "    <br>\n",
    "    with the goal to generate the next character after <font face='courier'>text</font>.<br>\n",
    "    <br>\n",
    "    Replace all placeholders <font face='courier'>None</font> as follows:<br>\n",
    "    <br>\n",
    "    First, apply <font face='courier'>preprocess</font> to encode <font face='courier'>text</font> and store the result in <font face='courier'>X_new</font>.<br>\n",
    "    <br>\n",
    "    Second, apply <font face='courier'>model.predict</font> the generate outputs given the input sequence <font face='courier'>X_new</font>.<br>\n",
    "    <br>\n",
    "    Store the output from the last time step in <font face='courier'>y_proba</font>.<br>\n",
    "    <br>\n",
    "    <font face='courier'>rescaled_logits</font> and <font face='courier'>char_id</font> are finally used to generate a one-element sample from  $\\{1,\\dots,\\mathrm{max\\_id}\\}$.<br>\n",
    "    <br>\n",
    "    The generated sample is the encoded version of the character to be generated.<br>\n",
    "    <br>\n",
    "    Hence, <font face='courier'>char_id.numpy()</font> will return the character itself.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = None\n",
    "    y_proba = None\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The function <font face='courier'>complete_text</font> takes a string <font face='courier'>text</font> and shall append <font face='courier'>n_char</font> subsequent characters generated by your RNN.<br>\n",
    "    <br>\n",
    "    Complete the function accordingly. The argument <font face='courier'>temperature</font> just needs to be passed to <font face='courier'>next_char</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += None\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Starting from the initial string <font face='courier'>'Hamlet'</font>, use <font face='courier'>complete_text</font> to generate a text with <font face='courier'>1000</font> characters.<br>\n",
    "    <br>\n",
    "    You can vary the argument <font face='courier'>temperature</font> to modify the distribution from which new characters are drawn.<br>\n",
    "    <br>\n",
    "    Values close to zero encourage characters that have a high probability according to the distribution generated by your RNN.<br>\n",
    "    <br>\n",
    "    If <font face='courier'>temperature</font> is too high, new characters are drawn according to a uniform distribution on the entire vocabulary, which is not desirable.<br>\n",
    "    <br>\n",
    "    You can, for example, try different values between <font face='courier'>0</font> and <font face='courier'>2</font> and evaluate generated texts based on how plausible they appear to you.<br>\n",
    "    <br>\n",
    "    Display your generated text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Submit your completed notebook not later than on January 15th, 2023.<br>\n",
    "    <br>\n",
    "    Optional: Train an RNN on a text corpus of your own choice and use your own RNN to generate text.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8 (default, Nov 16 2020, 16:55:22) \n[GCC 4.8.5 20150623 (Red Hat 4.8.5-44)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
